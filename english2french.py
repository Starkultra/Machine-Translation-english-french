# -*- coding: utf-8 -*-
"""english2french.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U0S84GeUaLPm8aBq74fLLrg_c3nR_yhw
"""

!pip install --upgrade tensorflow-gpu==2.0

# install nltk
!pip install nltk
# install gensim
!pip install gensim
# install spacy
!pip install spacy
!pip install plotly

import nltk
nltk.download('punkt')

from collections import Counter
import operator
import plotly.express as px
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud, STOPWORDS
import nltk
import re
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from tensorflow.keras.preprocessing.text import one_hot, Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, TimeDistributed, RepeatVector, Embedding, Input, LSTM, Conv1D, MaxPool1D, Bidirectional
from tensorflow.keras.models import Model

df_en = pd.read_csv('small_vocab_en.csv', sep = '/t', names = ['english'])
df_fr= pd.read_csv('small_vocab_fr.csv', sep = '/t', names = ['french'])

df_en

df_fr

df_en.info()

df_fr.info()

df_en.isnull().sum()

df_fr.isnull().sum()

df=pd.concat([df_en,df_fr],axis=1)

df

print("Total english Records : {} Total french Records :{}".format(len(df['english']),len(df['french'])))

"""**DATA CLEANING**"""

# download nltk packages
nltk.download('punkt')

# download stopwords
nltk.download("stopwords")

# function to remove punctuations
def remove_punc(x):
  return re.sub('[!#?,.:";]', '', x)

df['french'] = df['french'].apply(remove_punc)
df['english'] = df['english'].apply(remove_punc)

english_words = []
french_words  = []

def get_unique(text,word_list):
    for i in text.split():
        if i not in word_list:
            word_list.append(i)
df['english'].apply(lambda x : get_unique(x,english_words))
df['french'].apply(lambda x : get_unique(x,french_words))
english_words

# number of unique words in french
french_words

total_english_words=len(english_words)
total_english_words

total_french_words=len(french_words)
total_french_words

"""**VISUALIZE CLEANED UP DATASET**"""

# Obtain list of all words in the dataset
words = []
for i in df['english']:
  for word in i.split():
    words.append(word)
    
words

# Obtain the total count of words
english_words_counts = Counter(words)
english_words_counts

# sort the dictionary by values
english_words_counts = sorted(english_words_counts.items(), key = operator.itemgetter(1), reverse = True)

english_words_counts

# append the values to a list for visualization purposes
english_words = []
english_counts = []
for i in range(len(english_words_counts)):
  english_words.append(english_words_counts[i][0])
  english_counts.append(english_words_counts[i][1])

english_words

english_counts

"""Plot barplot using plotly"""

fig = px.bar(x = english_words, y = english_counts)
fig.show()

"""Word Cloud for text that is Real"""

plt.figure(figsize = (20,20)) 
wc = WordCloud(max_words = 2000, width = 1600, height = 800 ).generate(" ".join(df.english))
plt.imshow(wc, interpolation = 'bilinear')

df.english[0]
nltk.word_tokenize(df.english[0])

maxlen_english = -1
for doc in df.english:
    tokens = nltk.word_tokenize(doc)
    if(maxlen_english < len(tokens)):
        maxlen_english = len(tokens)
print("The maximum number of words in any document = ", maxlen_english)

fr_words=[]
for words in df['french']:
    for word in words.split():
        fr_words.append(word)
fr_words

french_word_count=Counter(fr_words)
french_word_count

french_words_counts=sorted(french_word_count.items(),key=operator.itemgetter(1),reverse=True)

french_words_counts

french_words=[]
french_counts=[]
for i in range(len(french_words_counts)):
    french_words.append(french_words_counts[i][0])
    french_counts.append(french_words_counts[i][1])

french_words

french_counts

fig=px.bar(x=french_words,y=french_counts)
fig.show()

plt.figure(figsize=(20,20))
wc=WordCloud(max_words=2000,width=1600,height=800).generate("".join(df['french']))
plt.imshow(wc,interpolation='bilinear')

maxlen_french=-1
for line in df.french:
    tokens=nltk.word_tokenize(doc)
    if (maxlen_french < len(tokens)):
        maxlen_french=len(tokens)
print('The maximun num of words in any document = ',maxlen_french)

"""**PREPARE THE DATA BY PERFORMING TOKENIZATION AND PADDING**"""

def tokenize_and_pad(x, maxlen):
  #  a tokenier to tokenize the words and create sequences of tokenized words
  tokenizer = Tokenizer(char_level = False)
  tokenizer.fit_on_texts(x)
  sequences = tokenizer.texts_to_sequences(x)
  padded = pad_sequences(sequences, maxlen = maxlen, padding = 'post')
  return tokenizer, sequences, padded

# tokenize and padding to the data 
# maxlen_english=15(It will pad zeros after 15 )
# maxlen_french=23
x_tokenizer, x_sequences, x_padded = tokenize_and_pad(df.english, maxlen_english)
y_tokenizer, y_sequences, y_padded = tokenize_and_pad(df.french,  maxlen_french)

# Total vocab size, since we added padding we add 1 to the total word count
english_vocab_size = total_english_words + 1
print("Complete English Vocab Size:", english_vocab_size)

# Total vocab size, since we added padding we add 1 to the total word count
french_vocab_size = total_french_words + 1
print("Complete French Vocab Size:", french_vocab_size)

print("The tokenized version for document\n", df.english[-1:].item(),"\n is : ", x_padded[-1:])

print("The tokenized version for document\n", df.french[-1:].item(),"\n is : ", y_padded[-1:])

# function to obtain the text from padded variables
def pad_to_text(padded, tokenizer):

    id_to_word = {id: word for word, id in tokenizer.word_index.items()}
    id_to_word[0] = ''

    return ' '.join([id_to_word[j] for j in padded])

pad_to_text(y_padded[0], y_tokenizer)

# Train test split
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x_padded, y_padded, test_size = 0.1)

"""**BUILD AND TRAIN THE MODEL**"""

# Sequential Model
model = Sequential()
# embedding layer
model.add(Embedding(english_vocab_size, 256, input_length = maxlen_english, mask_zero = True))
# encoder
model.add(LSTM(256))
# decoder
model.add(RepeatVector(maxlen_french))
model.add(LSTM(256, return_sequences= True ))
model.add(TimeDistributed(Dense(french_vocab_size, activation ='softmax')))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

# change the shape of target from 2D to 3D
y_train = np.expand_dims(y_train, axis = 2)
y_train.shape

model.fit(x_train, y_train, batch_size=1024, validation_split= 0.1, epochs=20)

"""**Got 92.5 % accuracy for 20 epoch :)**"""

# save the model
model.save("weights.h5")

"""**ASSESS TRAINED MODEL PERFORMANCE**"""

x_test.shape

x_test[0]

y_predict=model.predict(x_test)

y_predict

# function to make prediction
def prediction(x, x_tokenizer = x_tokenizer, y_tokenizer = y_tokenizer):
    predictions = model.predict(x)[0]
    id_to_word = {id: word for word, id in y_tokenizer.word_index.items()}
    id_to_word[0] = ''
    return ' '.join([id_to_word[j] for j in np.argmax(predictions,1)])

for i in range(10):

  print('Original English word - {}\n'.format(pad_to_text(x_test[i], x_tokenizer)))
  print('Original French word - {}\n'.format(pad_to_text(y_test[i], y_tokenizer)))
  print('Predicted French word - {}\n\n\n\n'.format(prediction(x_test[i:i+1])))